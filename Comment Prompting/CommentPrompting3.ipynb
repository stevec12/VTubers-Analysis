{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHHlMi+KQ04qaP2D0Sfo5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevec12/VTubers-Analysis/blob/main/CommentPrompting3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comment Prompting\n",
        "This Jupyter notebook looks at training a basic transformer to provide responses to prompts based on how a YouTuber's comments would likely reply.\n",
        "\n",
        "The YouTuber chosen is for the demo is [Ceres Fauna](!https://www.youtube.com/channel/UCO_aKKYxn4tvrqPjcTzZ6EQ), an English streamer with predominantly English comments.\n",
        "\n",
        "The channel ID is `UCO_aKKYxn4tvrqPjcTzZ6EQ`."
      ],
      "metadata": {
        "id": "R9P3PkBKBAqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction\n",
        "The `YouTube Data API v3` can be used for this task, and an account-linked API-key can be obtained using your personal Google (Developer) Account."
      ],
      "metadata": {
        "id": "nKHKEv3UCDd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import googleapiclient.discovery\n",
        "import googleapiclient.errors\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter"
      ],
      "metadata": {
        "id": "zU9QbdDpRqeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c07701-22dc-4715-a358-1343cea328ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input API Key: \")\n",
        "api_key = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633vu2o7P3Rs",
        "outputId": "ec005340-c894-4295-8b87-ac251db33fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input API Key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VNtmD6yA9KH"
      },
      "outputs": [],
      "source": [
        "# Input target channel, example is @CeresFauna\n",
        "channelID = 'UCO_aKKYxn4tvrqPjcTzZ6EQ'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=api_key)"
      ],
      "metadata": {
        "id": "bnUk47X1WPjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_uploadedID(channelID):\n",
        "  request = youtube.channels().list(\n",
        "      part=\"contentDetails\",\n",
        "      id=channelID\n",
        "    )\n",
        "  response = request.execute()\n",
        "\n",
        "  return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']"
      ],
      "metadata": {
        "id": "jkoA1_GiWU09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploadedID=find_uploadedID(channelID)"
      ],
      "metadata": {
        "id": "7LkCawW_WcQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_uploaded(uploadedID):\n",
        "  videoIDs = []\n",
        "  request = youtube.playlistItems().list(\n",
        "        part=\"contentDetails\",\n",
        "        playlistId = uploadedID,\n",
        "        maxResults = 50\n",
        "  )\n",
        "  response = request.execute()\n",
        "  for item in response['items']:\n",
        "    videoIDs.append(item['contentDetails']['videoId'])\n",
        "  while('nextPageToken' in response):\n",
        "    request=youtube.playlistItems().list(\n",
        "        part='contentDetails',\n",
        "        playlistId=uploadedID,\n",
        "        pageToken=response['nextPageToken'],\n",
        "        maxResults=50)\n",
        "    response = request.execute()\n",
        "    for item in response['items']:\n",
        "      videoIDs.append(item['contentDetails']['videoId'])\n",
        "\n",
        "  return videoIDs"
      ],
      "metadata": {
        "id": "oefayJeWXusJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded=find_uploaded(uploadedID)"
      ],
      "metadata": {
        "id": "pwn7iZ7MXw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_comments(videoID : str) -> pd.DataFrame:\n",
        "  '''\n",
        "  Given a videoID, return a pandas DataFrame with video info\n",
        "  '''\n",
        "  column_names = ['videoID','isTopLevel','topLevelID','commentID','authorDisplayName',\n",
        "                  'likeCount','publishedAt','totalReplyCount','textOriginal']\n",
        "\n",
        "  row_list = [] # Used to create list of dict of rows before conversion to dataframe, faster\n",
        "  pageToken=''\n",
        "  while(True):\n",
        "    request=youtube.commentThreads().list(\n",
        "        part=\"id,snippet,replies\",\n",
        "        videoId=videoID,\n",
        "        pageToken=pageToken,\n",
        "        maxResults=100\n",
        "    )\n",
        "    try:\n",
        "      response=request.execute()\n",
        "    except googleapiclient.errors.HttpError:\n",
        "      break\n",
        "\n",
        "    for commentThread in response['items']:\n",
        "      # write top level comment\n",
        "      topLevelID=commentThread['snippet']['topLevelComment']['id']\n",
        "      commentID=topLevelID\n",
        "      authorDisplayName=commentThread['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
        "      likeCount=commentThread['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "      publishedAt=commentThread['snippet']['topLevelComment']['snippet']['publishedAt']\n",
        "      totalReplyCount=commentThread['snippet']['totalReplyCount']\n",
        "      textOriginal=commentThread['snippet']['topLevelComment']['snippet']['textOriginal']\n",
        "\n",
        "      row_list.append({'videoID':videoID,'isTopLevel':True,'topLevelID':topLevelID,\n",
        "                      'commentID':commentID,'authorDisplayName':authorDisplayName,\n",
        "                      'likeCount':likeCount,'publishedAt':publishedAt,\n",
        "                      'totalReplyCount':totalReplyCount,'textOriginal':textOriginal})\n",
        "\n",
        "      # If any replies, write them as well\n",
        "      if 'replies' in commentThread:\n",
        "        for reply in commentThread['replies']['comments']:\n",
        "          commentID=reply['id']\n",
        "          authorDisplayName=reply['snippet']['authorDisplayName']\n",
        "          likeCount=reply['snippet']['likeCount']\n",
        "          publishedAt=reply['snippet']['publishedAt']\n",
        "          textOriginal=reply['snippet']['textOriginal']\n",
        "\n",
        "          row_list.append({'videoID':videoID,'isTopLevel':False,'topLevelID':topLevelID,\n",
        "                           'commentID':commentID,'authorDisplayName':authorDisplayName,\n",
        "                           'likeCount':likeCount,'publishedAt':publishedAt,\n",
        "                           'totalReplyCount':totalReplyCount,'textOriginal':textOriginal})\n",
        "\n",
        "    if 'nextPageToken' not in response:\n",
        "      break\n",
        "    else:\n",
        "      pageToken=response['nextPageToken']\n",
        "\n",
        "  return pd.DataFrame(row_list, columns=column_names)\n"
      ],
      "metadata": {
        "id": "msEQBZdbYkRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uploaded_comments_to_excel(file_name, uploaded = uploaded):\n",
        "  '''\n",
        "  Writes all comments in the Uploaded playlist to an excel file, as a single\n",
        "  worksheet.\n",
        "  '''\n",
        "  column_names = ['videoID','isTopLevel','topLevelID','commentID','authorDisplayName',\n",
        "                  'likeCount','publishedAt','totalReplyCount','textOriginal']\n",
        "  comment_df = get_video_comments(uploaded[0])\n",
        "\n",
        "  for videoID in uploaded[1:]:\n",
        "    comment_df = pd.concat([comment_df, get_video_comments(videoID)])\n",
        "\n",
        "  comment_df.to_excel(file_name, engine='xlsxwriter', index=False)\n"
      ],
      "metadata": {
        "id": "ZSJKXRcXsCWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_comments_to_excel('ceres_fauna_comments_10_27_2023.xlsx')"
      ],
      "metadata": {
        "id": "ccL9tBPR1sE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Data for TensorFlow\n",
        "Preparing the data using TensorFlow preprocessing layers.\n",
        "\n",
        "Here, we use the `ceres_fauna_comments_10_27_2023.xlsx` excel file generated earlier."
      ],
      "metadata": {
        "id": "I6_e9cNGbCCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "!pip install tensorflow_text\n",
        "import tensorflow_text as text\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC6gkYsRbBvh",
        "outputId": "657ae888-7156-4814-9765-8998c2a977c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.15.0)\n",
            "Requirement already satisfied: tensorflow<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow_text) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: tensorflow_text\n",
            "Successfully installed tensorflow_text-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into a pandas DataFrame\n",
        "comments_df = pd.read_excel('ceres_fauna_comments_10_27_2023.xlsx')"
      ],
      "metadata": {
        "id": "MKNeeBITas9V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We filter out comments that are not at least two seperate words (i.e. do not have at least one space).\n",
        "\n",
        "This also has the effect of filtering out many purely non-English sentences.\n",
        "\n",
        "We also shuffle the resulting tensor with `seed=12`."
      ],
      "metadata": {
        "id": "JE_tKou0mcDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_word_indices = np.char.find(comments_df['textOriginal'].to_numpy(dtype='str'), \" \") > -1\n",
        "multiple_word_series = comments_df.copy().loc[multiple_word_indices]['textOriginal']\n",
        "\n",
        "comments_tensor = tf.convert_to_tensor(multiple_word_series.to_numpy(dtype='str'), dtype='string')\n",
        "comments_tensor = tf.random.shuffle(comments_tensor, seed=12)"
      ],
      "metadata": {
        "id": "qtITGgm8QC-P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into train, validation, and test splits.\n",
        "\n",
        "For reasonable training times, we use a 60/10/30 split."
      ],
      "metadata": {
        "id": "gc35nzFoh27I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_proportion = 0.6\n",
        "valid_proportion = 0.1"
      ],
      "metadata": {
        "id": "AEpyeNIdXuwQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we prepare the training data to generate a vocabulary needed for the BERT tokenizer."
      ],
      "metadata": {
        "id": "7P6w2pgamzX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comment_ds = tf.data.Dataset.from_tensor_slices(comments_tensor)\n",
        "\n",
        "train_split = int(np.floor(train_proportion*len(comment_ds)))\n",
        "train_ds = comment_ds.take(train_split)"
      ],
      "metadata": {
        "id": "hQZ9PIJRcDRJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate vocabulary using [subword tokenizers](https://www.tensorflow.org/text/guide/subwords_tokenizer) tutorial."
      ],
      "metadata": {
        "id": "JvT9hq5SfjSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "metadata": {
        "id": "r6VcV382kgH1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = 'vocab3.txt'"
      ],
      "metadata": {
        "id": "UHHDzYZpyWs-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_ds.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n",
        "\n",
        "# Save vocab to file\n",
        "with open(vocab_file, 'w') as f:\n",
        "  for token in vocab:\n",
        "    print(token, file=f)"
      ],
      "metadata": {
        "id": "lMFV_8B6gBSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9936bc-add1-47d7-ab4a-0d10f3c2ba14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 33s, sys: 270 ms, total: 1min 33s\n",
            "Wall time: 1min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Track vocab_size\n",
        "vocab_size = 0\n",
        "with open(vocab_file, \"rb\") as f:\n",
        "    vocab_size = sum(1 for _ in f)"
      ],
      "metadata": {
        "id": "D7B3Qx6-xshl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = text.BertTokenizer(vocab_file, **bert_tokenizer_params)"
      ],
      "metadata": {
        "id": "lWW4j7rpud-8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free unused memory before next steps\n",
        "del comments_df\n",
        "del comment_ds"
      ],
      "metadata": {
        "id": "9mk_nmliC_Ue"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we can prepare the dataset used in the Transformer model.\n",
        "\n",
        "The data is formed as a generator to scale better by not hitting GPU ram limits.\n",
        "\n",
        "Then tokenize, trim (to `MAX_TOKENS`), and pad the sentences, as well as form into `(prompt, teacher), label` Datasets where:\n",
        "* `prompt` is a random proper subset of the sentence, beginning from the first word, bracketed by `[START]`,`[END]` tokens\n",
        "* `label` is the entire trimmed sentence, followed by `[END]` token\n",
        "* `teacher` is the `label` left-shifted by on token, beginning with the `[START]` token"
      ],
      "metadata": {
        "id": "QshfKIgWC-k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 128\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "Y8sIifsFVN2C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1,1], START)\n",
        "  ends = tf.fill([count,1,1], END)\n",
        "\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "metadata": {
        "id": "DmoG18ibUxC0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class commentSequence(tf.keras.utils.Sequence):\n",
        "  def __init__(self, comment_tensor, batch_size=BATCH_SIZE, max_tokens=MAX_TOKENS):\n",
        "    self.comment_tensor = comment_tensor\n",
        "    self.batch_size = batch_size\n",
        "    self.max_tokens = max_tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(self.comment_tensor.shape[0]/float(self.batch_size)))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    low = idx*self.batch_size\n",
        "    high = min(low+self.batch_size, self.comment_tensor.shape[0])\n",
        "    batch_size = high-low # gives current batch size\n",
        "\n",
        "    tokens = tokenizer.tokenize(self.comment_tensor[low:high])[:,:self.max_tokens-1,:]\n",
        "\n",
        "    # Create random length prompts, beginning from start of sentence\n",
        "    prompt = tokens[:,:-1,:] # Shape: (batch_size, <= max_tokens-2, 1)\n",
        "    token_lens = tf.cast(prompt.row_lengths()-1, dtype='float32') # Keep at least 1 token outside prompt\n",
        "    prompt_lens = tf.floor(tf.random.uniform([batch_size], tf.zeros_like(token_lens), token_lens))\n",
        "    prompt_lens = tf.squeeze(tf.cast(prompt_lens, dtype='int32'))\n",
        "\n",
        "    prompt = tf.squeeze(prompt.to_tensor(shape=(batch_size, self.max_tokens-2, 1)), axis=2)\n",
        "    prompt = tf.RaggedTensor.from_tensor(prompt,prompt_lens)[:,:,tf.newaxis]\n",
        "\n",
        "    # Add [START], [END] tokens\n",
        "    prompt = add_start_end(prompt)\n",
        "    teacher = add_start_end(tokens)[:,:-1,:]\n",
        "    label = add_start_end(tokens)[:,1:,:]\n",
        "\n",
        "    # 0-Pad, convert to dense tensor, then form shape (batch_size, max_tokens)\n",
        "    prompt = tf.squeeze(prompt.to_tensor(shape=(batch_size,self.max_tokens,1)))\n",
        "    teacher = tf.squeeze(teacher.to_tensor(shape=(batch_size,self.max_tokens,1)))\n",
        "    label = tf.squeeze(label.to_tensor(shape=(batch_size,self.max_tokens,1)))\n",
        "\n",
        "    return (prompt, teacher), label\n",
        ""
      ],
      "metadata": {
        "id": "LMIUKUNwqt91"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = commentSequence(comment_tensor = comments_tensor[:train_split])"
      ],
      "metadata": {
        "id": "lGIEPowD4l_L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_count = int(np.floor(valid_proportion*len(comments_tensor)))\n",
        "valid_gen = commentSequence(comment_tensor = comments_tensor[train_split:train_split+valid_count])"
      ],
      "metadata": {
        "id": "Q24PLV527DdO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a singular batch from the train data as an example."
      ],
      "metadata": {
        "id": "JY7eV-WH5Jjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(input,teacher), label = train_gen[0]\n",
        "print(input.shape)\n",
        "print(teacher.shape)\n",
        "print(label.shape)"
      ],
      "metadata": {
        "id": "--IoHCaM4CfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d20886-62a4-4774-9ad9-c6aa0b3fc789"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128)\n",
            "(64, 128)\n",
            "(64, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input[0])\n",
        "print(teacher[0])\n",
        "print(label[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LteMAF345NfI",
        "outputId": "eda727a8-7549-4b61-a954-9683570cebc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[   2 3513    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(128,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[   2 3513 1009 2193   17   60 1063 4504   17 2157  990 1107 1787  992\n",
            " 1017 2181 4504 1150  997 1026 2321 5835 6516 1019   15 1301   15 2400\n",
            "   15 1010 1054 1194 4253   17 2439 1026 4494 1050 1663  993  992 1017\n",
            "   10   61 2181 1468 1046 1123   17    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(128,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[3513 1009 2193   17   60 1063 4504   17 2157  990 1107 1787  992 1017\n",
            " 2181 4504 1150  997 1026 2321 5835 6516 1019   15 1301   15 2400   15\n",
            " 1010 1054 1194 4253   17 2439 1026 4494 1050 1663  993  992 1017   10\n",
            "   61 2181 1468 1046 1123   17    3    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(128,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert tokens to vectors with a `tf.keras.layers.Embedding` layer and add positional encoding."
      ],
      "metadata": {
        "id": "hYz1JD16eWZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "wJyTvASxeVO-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "octsiexdfjQx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "8Qk7KH4D5XQg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  '''def __init__(self, **kwargs):\n",
        "    print('Initializing CrossAttention')\n",
        "    super().__init__(self, **kwargs)'''\n",
        "\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query = x,\n",
        "        key = context,\n",
        "        value = context,\n",
        "        return_attention_scores = True\n",
        "    )\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x,attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "7aSUK3LaafK8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query = x,\n",
        "        key = x,\n",
        "        value = x\n",
        "    )\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "WAJfmyXYFd1z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query = x,\n",
        "        key = x,\n",
        "        value = x,\n",
        "        use_causal_mask = True\n",
        "    )\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "09Pw5iY0CT5h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model),\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x,self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ubYe9mNEFUGo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*,d_model,num_heads,dff,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "    self.ffn = FeedForward(d_model,dff)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qn8SHOyGNFgk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size = vocab_size, d_model = d_model\n",
        "    )\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self,x):\n",
        "    # `x` is token-IDs shape: (batch_size, seq_len)\n",
        "    x = self.pos_embedding(x) # Shape '(batch_size, seq_len, d_model)'.\n",
        "\n",
        "    # Add dropout\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x # Shape `(batch_size, seq_length, d_model)`"
      ],
      "metadata": {
        "id": "mjYtpLjSQOvz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*,d_model,num_heads,dff,dropout_rate=0.1):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads = num_heads,\n",
        "        key_dim = d_model,\n",
        "        dropout = dropout_rate\n",
        "    )\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads = num_heads,\n",
        "        key_dim = d_model,\n",
        "        dropout = dropout_rate\n",
        "    )\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x) # Shape `(batch_size, seq_len, d_model)`\n",
        "    return x"
      ],
      "metadata": {
        "id": "WAaVdSSAU2NA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*,num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.dec_layers[i](x,context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # shape of x is (batch_size, target_seq_len, d_model)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "9SnFsXHNM7MA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                           dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                           dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To support Keras model '.fit', pass all inputs as first argument\n",
        "    context, x = inputs\n",
        "\n",
        "    context = self.encoder(context) # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context) # (batch_size, target_len, d_model)\n",
        "\n",
        "    logits = self.final_layer(x) # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop keras mask, so it doesn't scale losses/metrics\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return final output and attention weights\n",
        "    return logits"
      ],
      "metadata": {
        "id": "cJ54y09FgegC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "DsECFvcbiyEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 6\n",
        "d_model = 128\n",
        "dff = 1024\n",
        "num_heads = 10\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "vtN0P31SiuF_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "MUoyRRlXjAOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                          dff=dff, input_vocab_size=vocab_size, target_vocab_size=vocab_size,\n",
        "                          dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "rvLmL2v9i81z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = transformer((input,teacher))\n",
        "print(teacher.shape)\n",
        "print(input.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKuqmIkDjVur",
        "outputId": "f4ff28f7-5fff-46c8-ba9d-9c1b41f386a8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128)\n",
            "(64, 128)\n",
            "(64, 128, 7881)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JwM1rpYwAFo",
        "outputId": "53d95111-66b1-4603-f0f9-2dd751231b32"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  6547584   \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  10505088  \n",
            "                                                                 \n",
            " dense_24 (Dense)            multiple                  1016649   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18069321 (68.93 MB)\n",
            "Trainable params: 18069321 (68.93 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "Uses Adam optimizer with original [Transformer paper](https://arxiv.org/abs/1706.03762) custom learning rate scheduler.\n",
        "\n",
        "$$lrate = d_{model}^{-0.5}*\\min\\left(step_{num}^{-0.5},step_{num}*warmup\\_steps^{-1.5}\\right)$$"
      ],
      "metadata": {
        "id": "TrPSjYbf1Gsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "metadata": {
        "id": "5RDe4LkzyGoc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9,\n",
        "                                     beta_2=0.98, epsilon=1e-9)"
      ],
      "metadata": {
        "id": "D6e28oFoP7jj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup padding mask for calculating loss properly\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none'\n",
        "  )\n",
        "  loss = loss_object(label,pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "j8e3YTkJRLbA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Checkpoint saving of weights after each epoch, then begin training."
      ],
      "metadata": {
        "id": "owey0AJrnljZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoint.ckpt'\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         verbose=1)"
      ],
      "metadata": {
        "id": "AqeCZSEomoQz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(loss=masked_loss, optimizer=optimizer,\n",
        "                    metrics=[masked_accuracy], run_eagerly=False)"
      ],
      "metadata": {
        "id": "eXbzm1U9T89v"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_gen, epochs=20, validation_data=valid_gen, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCDHhuTPUJ_z",
        "outputId": "7b6c77b0-2107-43e0-b65c-d6e9793faa3c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 6.5416 - masked_accuracy: 0.1143\n",
            "Epoch 1: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 654s 723ms/step - loss: 6.5416 - masked_accuracy: 0.1143 - val_loss: 4.8323 - val_masked_accuracy: 0.2466\n",
            "Epoch 2/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 4.2009 - masked_accuracy: 0.3280\n",
            "Epoch 2: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 606s 721ms/step - loss: 4.2009 - masked_accuracy: 0.3280 - val_loss: 3.4936 - val_masked_accuracy: 0.4320\n",
            "Epoch 3/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 3.3855 - masked_accuracy: 0.4447\n",
            "Epoch 3: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 607s 723ms/step - loss: 3.3855 - masked_accuracy: 0.4447 - val_loss: 3.0721 - val_masked_accuracy: 0.4901\n",
            "Epoch 4/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 3.0422 - masked_accuracy: 0.4929\n",
            "Epoch 4: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 643s 765ms/step - loss: 3.0422 - masked_accuracy: 0.4929 - val_loss: 2.8039 - val_masked_accuracy: 0.5283\n",
            "Epoch 5/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.9184 - masked_accuracy: 0.5082\n",
            "Epoch 5: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 641s 763ms/step - loss: 2.9184 - masked_accuracy: 0.5082 - val_loss: 2.7400 - val_masked_accuracy: 0.5361\n",
            "Epoch 6/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.7867 - masked_accuracy: 0.5251\n",
            "Epoch 6: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 606s 721ms/step - loss: 2.7867 - masked_accuracy: 0.5251 - val_loss: 2.6515 - val_masked_accuracy: 0.5472\n",
            "Epoch 7/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.6652 - masked_accuracy: 0.5411\n",
            "Epoch 7: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 646s 769ms/step - loss: 2.6652 - masked_accuracy: 0.5411 - val_loss: 2.5655 - val_masked_accuracy: 0.5598\n",
            "Epoch 8/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.5679 - masked_accuracy: 0.5537\n",
            "Epoch 8: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 608s 724ms/step - loss: 2.5679 - masked_accuracy: 0.5537 - val_loss: 2.5199 - val_masked_accuracy: 0.5654\n",
            "Epoch 9/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.5831 - masked_accuracy: 0.5469\n",
            "Epoch 9: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 611s 727ms/step - loss: 2.5831 - masked_accuracy: 0.5469 - val_loss: 2.5224 - val_masked_accuracy: 0.5631\n",
            "Epoch 10/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.5127 - masked_accuracy: 0.5558\n",
            "Epoch 10: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 607s 722ms/step - loss: 2.5127 - masked_accuracy: 0.5558 - val_loss: 2.5318 - val_masked_accuracy: 0.5609\n",
            "Epoch 11/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.4239 - masked_accuracy: 0.5694\n",
            "Epoch 11: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 608s 724ms/step - loss: 2.4239 - masked_accuracy: 0.5694 - val_loss: 2.4538 - val_masked_accuracy: 0.5732\n",
            "Epoch 12/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.3818 - masked_accuracy: 0.5739\n",
            "Epoch 12: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 608s 723ms/step - loss: 2.3818 - masked_accuracy: 0.5739 - val_loss: 2.4414 - val_masked_accuracy: 0.5771\n",
            "Epoch 13/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.3678 - masked_accuracy: 0.5750\n",
            "Epoch 13: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 604s 719ms/step - loss: 2.3678 - masked_accuracy: 0.5750 - val_loss: 2.4215 - val_masked_accuracy: 0.5767\n",
            "Epoch 14/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.3463 - masked_accuracy: 0.5764\n",
            "Epoch 14: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 606s 721ms/step - loss: 2.3463 - masked_accuracy: 0.5764 - val_loss: 2.4376 - val_masked_accuracy: 0.5751\n",
            "Epoch 15/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2889 - masked_accuracy: 0.5844\n",
            "Epoch 15: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 605s 720ms/step - loss: 2.2889 - masked_accuracy: 0.5844 - val_loss: 2.4007 - val_masked_accuracy: 0.5816\n",
            "Epoch 16/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2850 - masked_accuracy: 0.5841\n",
            "Epoch 16: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 606s 721ms/step - loss: 2.2850 - masked_accuracy: 0.5841 - val_loss: 2.4206 - val_masked_accuracy: 0.5783\n",
            "Epoch 17/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2769 - masked_accuracy: 0.5839\n",
            "Epoch 17: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 605s 721ms/step - loss: 2.2769 - masked_accuracy: 0.5839 - val_loss: 2.4179 - val_masked_accuracy: 0.5798\n",
            "Epoch 18/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2586 - masked_accuracy: 0.5858\n",
            "Epoch 18: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 606s 722ms/step - loss: 2.2586 - masked_accuracy: 0.5858 - val_loss: 2.4107 - val_masked_accuracy: 0.5798\n",
            "Epoch 19/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2433 - masked_accuracy: 0.5870\n",
            "Epoch 19: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 605s 720ms/step - loss: 2.2433 - masked_accuracy: 0.5870 - val_loss: 2.4233 - val_masked_accuracy: 0.5774\n",
            "Epoch 20/20\n",
            "840/840 [==============================] - ETA: 0s - loss: 2.2398 - masked_accuracy: 0.5859\n",
            "Epoch 20: saving model to training_3/checkpoint.ckpt\n",
            "840/840 [==============================] - 608s 724ms/step - loss: 2.2398 - masked_accuracy: 0.5859 - val_loss: 2.4202 - val_masked_accuracy: 0.5794\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fbdcad5cb50>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading model weights manually."
      ],
      "metadata": {
        "id": "1udyjLoUmaN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load weights\n",
        "transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                          dff=dff, input_vocab_size=vocab_size, target_vocab_size=vocab_size,\n",
        "                          dropout_rate=dropout_rate)\n",
        "transformer.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSQ2S3jfmQwU",
        "outputId": "9373338b-9495-45eb-cfc9-927674958b6d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7be730267130>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Testing\n",
        "Not expected to be especially different than the validation accuracy, but could be interesting regardless.\n",
        "\n",
        "Only a fraction of the test set is used to save time."
      ],
      "metadata": {
        "id": "d3L3_pEHPJka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_fraction = 0.1\n",
        "test_count = tf.cast(tf.floor(len(comment_ds)*test_fraction), dtype='int64')\n",
        "test_gen = commentSequence(comment_tensor = comments_tensor[train_split+valid_count:train_split+valid_count+test_count],\n",
        "                           batch_size = test_count)\n",
        "(test_prompt, test_teacher), test_label = test_gen[0]"
      ],
      "metadata": {
        "id": "djN8AfpOPpfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test_loss, test_acc = transformer.evaluate(x=(test_prompt,test_teacher), y=test_label, verbose=2)"
      ],
      "metadata": {
        "id": "Z-LwJPKVPLn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Inference\n",
        "Create a model to generate comments from prompts:\n",
        "* Encode prompt with `tokenizer`, trim, add `[START],[END]`, then pad - this is the encoder input\n",
        "* calculate padding masks and look-ahead masks\n",
        "* `decoder` outputs preds by looking at `encoder` output and own output\n",
        "* Concatenate predicted token to decoder input and pass to of decoder\n",
        "* Decoder predicts next token based on previous tokens it predicted\n",
        "\n",
        "Add a `temperature` call argument that allows variability in the response\n",
        "* `temperture <= 0` always gives the most likely prediction"
      ],
      "metadata": {
        "id": "6TLa8PsBtJDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemperatureCommentator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, temperature = 0.0, max_length=128):\n",
        "    '''\n",
        "    Temperature = 0 equivalent to always selecting most likely prediction\n",
        "    '''\n",
        "    # Add '[START]' and '[END]' tokens to input sentence\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.tokenize(sentence)[:,:MAX_TOKENS-2,:]\n",
        "    sentence = tf.squeeze(add_start_end(sentence).to_tensor(shape=(1,MAX_TOKENS,1)),axis=2)\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # Init output with '[START]' token\n",
        "    out = self.tokenizers.tokenize(tf.constant(['']))\n",
        "    start_end = add_start_end(out)[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # 'tf.TensorArray' required so dynamic-loop traced by tf.function\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      output = tf.squeeze(output, axis=1)\n",
        "      predictions = self.transformer([encoder_input, output], training = False)\n",
        "\n",
        "      # Select last token from `seq_len` dimension\n",
        "      predictions = tf.squeeze(predictions[:,-1:,:], axis=0) # Shape `(batch_size, 1, vocab_size)`\n",
        "\n",
        "      # Adjust randomness for `temperature`, with 0 (or less) offering no randomness\n",
        "      predicted_id = None\n",
        "      if (temperature > 0):\n",
        "        predicted_id = tf.random.categorical(predictions/temperature, num_samples=1)\n",
        "      else:\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)[tf.newaxis]\n",
        "\n",
        "      # Concatenate `predicted_id` to output given to decoder as input\n",
        "      output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.squeeze(tf.transpose(output_array.stack()), axis=0)\n",
        "    # output shape `(1,tokens)`\n",
        "    # text removes the `[START]`,`[END]` tokens\n",
        "    text = tf.strings.reduce_join(self.tokenizers.detokenize(output)[0][1:-1], axis=0, separator=\" \") # Shape: `()`\n",
        "\n",
        "    tokens = self.tokenizers.detokenize(output)[0]\n",
        "    # `tf.function` prevents usage of attention_wieghts calculated\n",
        "    # on last iteration of loop - recalc. outside of loop\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "5Q8ijKofneN_"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature_commentator = TemperatureCommentator(tokenizer, transformer)"
      ],
      "metadata": {
        "id": "oQ626hAsUynn"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I miss'\n",
        "output_text, output_tokens, attention_weights = temperature_commentator(tf.constant(sentence),0)\n",
        "print_comment(sentence, output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNVJTuiUYYWZ",
        "outputId": "5f194673-612f-4881-ef18-6b5dee203cb7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : I miss\n",
            "Prediction     : i miss fauna . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I miss'\n",
        "output_text, output_tokens, attention_weights = temperature_commentator(tf.constant(sentence),1.5)\n",
        "print_comment(sentence, output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaO0xZ4K9o8C",
        "outputId": "60720892-57aa-4733-e4df-b5d4e0cad852"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : I miss\n",
            "Prediction     : i miss you my sap never head lit throwing vacation evidence * extinct 9 christ anyone isn ' t arm buttons dig the class immortal floor ! coconut ur drunk squeeb loved expecting a full school mother nature or shiny abilities 💚💚💚💚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Take care of'\n",
        "output_text, output_tokens, attention_weights = temperature_commentator(tf.constant(sentence),0.5)\n",
        "print_comment(sentence, output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6YgSMyl30pE",
        "outputId": "d0dd3fd7-9f7c-4753-f183-62e93965648c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Take care of\n",
            "Prediction     : [START] take care of mother nature . [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Model"
      ],
      "metadata": {
        "id": "uL0qPbTdtof3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExportCommentator(tf.Module):\n",
        "  def __init__(self, commentator):\n",
        "    self.commentator = temperature_commentator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string),\n",
        "                                tf.TensorSpec([], tf.float32)])\n",
        "  def __call__(self, sentence, temperature=0):\n",
        "    (result, tokens, attention_weights) = self.commentator(sentence = sentence, temperature = temperature, max_length=MAX_TOKENS)\n",
        "    return result"
      ],
      "metadata": {
        "id": "SNeTrrtJts9r"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_commentator = ExportCommentator(temperature_commentator)"
      ],
      "metadata": {
        "id": "Iod_-0qhvjUI"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_commentator('Give me')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W8o6CB4XE6W",
        "outputId": "8f90363f-d657-44dd-93f0-aa3004ce12e5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'give me a hug .'>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(export_commentator, export_dir = 'export_commentator3')"
      ],
      "metadata": {
        "id": "j2urfLrsvcPS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_commentator = tf.saved_model.load(export_dir = 'export_commentator3')"
      ],
      "metadata": {
        "id": "OU_L3YwQiY-Q"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_commentator('Take care of')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WaGRexFjpSE",
        "outputId": "d2c0d5cc-c532-44df-d2c0-81ee5e9a9e5a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'take care of yourself , fauna !'>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    }
  ]
}
